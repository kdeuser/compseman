{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost's hyperparameters\n",
    "XGBoost has a multitude of tuning parameters that are split into three categories:\n",
    "- General Parameters, which guide the overall functioning\n",
    "- Booster Parameters, which guide the indiviual boosters at each step\n",
    "- Learning Task Parameters, which guide the opitmization performed\n",
    "Not all are important to our project, but we will need to spend some time looking at the tuning parameters to figure out the best ones for out project.\n",
    "\n",
    "There are too many for us to list all, butt some of the most common ones, and most likely that we will use, are:\n",
    "- nthread\n",
    "- seed\n",
    "- silent\n",
    "- subsample\n",
    "- max_delta_step\n",
    "- missing\n",
    "- scale_pos_weight\n",
    "- booster\n",
    "- gbtree\n",
    "- max_depth\n",
    "- n_estimators\n",
    "- colsample_bytree\n",
    "- reg_lambda\n",
    "- colsample_bylevel\n",
    "- objective\n",
    "- leaning_rate\n",
    "- base_score\n",
    "- min_child_weight\n",
    "- reg_alpha\n",
    "- gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('/users/kdeuser/Desktop/CMPU366/FinalProject/mbti_1.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarized MBTI list:\n",
      "[[0 0 1 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "Back to MBTI:  \n",
      "['INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'INTJ' 'INFJ' 'INTJ' 'INFJ' 'INTP']\n",
      "Original MBTI: \n",
      "0    INFJ\n",
      "1    ENTP\n",
      "2    INTP\n",
      "3    INTJ\n",
      "4    ENTJ\n",
      "5    INTJ\n",
      "6    INFJ\n",
      "7    INTJ\n",
      "8    INFJ\n",
      "9    INTP\n",
      "Name: type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## set all MBTI letters to correspond to 0 and 1s\n",
    "mbtipersonalities = {'I':0, 'E':1, 'N':0, 'S':1, 'T':0, 'F':1, 'J':0, 'P':1}\n",
    "## format of MBTI perdonality types\n",
    "mbtip_list = [{0:'I', 1:'E'}, {0:'N', 1:'E'}, {0:'T', 1:'F'}, {0:'J', 1:'P'}]\n",
    "\n",
    "## make mbti personalities to binary vectors\n",
    "def mbti_to_bin_vec(mbti_pers):\n",
    "    return [mbtipersonalities[i] for i in mbti_pers]\n",
    "\n",
    "## change back from binary vectors to the mbti personalities\n",
    "def bin_vec_to_mbti(mbti_pers):\n",
    "    ## needs to be a string\n",
    "    s = \"\"\n",
    "    for i, j in enumerate(mbti_pers):\n",
    "        s += mbtip_list[i][j]\n",
    "    return s\n",
    "\n",
    "check = data.head(10)\n",
    "mbti_binvec  = np.array([mbti_to_bin_vec(i) for i in check.type])\n",
    "print(\"Binarized MBTI list:\\n%s\"% mbti_binvec)\n",
    "\n",
    "binvec_mbti = np.array([bin_vec_to_mbti(i) for i in mbti_binvec])\n",
    "print(\"Back to MBTI:  \\n%s\" % binvec_mbti)\n",
    "\n",
    "##check with og data.head\n",
    "print(\"Original MBTI: \\n%s\" % data.head(10).type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to preprocss the data we have to make everything lowercase, remove\n",
    "# nonalphabetic words, urls and overly common words (e.g. a, the, at, etc.)\n",
    "# and lemmatice the words\n",
    "\n",
    "### ADD REMOVEING STOPPWORDS and mbti types\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "## list of mbti types\n",
    "mbti_types_list = ['ESTP', 'ESFP', 'ENFP', 'ENTP', 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ',\n",
    "                  'ISTJ', 'ISFJ', 'INFJ', 'INTJ', 'ISTP', 'ISFP', 'INFP', 'INTP']\n",
    "mbti_types_list = [i.lower() for i in mbti_types_list]\n",
    "\n",
    "##choose stopwords\n",
    "stopWOrds = stopwords.words(\"english\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(data, remove_mbti=True, remove_stopwords=True):\n",
    "    personalities = []\n",
    "    posts = []\n",
    "    datalength = len(data)\n",
    "    i = 0\n",
    "    \n",
    "    ## iterate through the rows \n",
    "    for row in data.iterrows():\n",
    "        i += 1\n",
    "        if (i % 500 == 0 or i ==1 or i == datalength):\n",
    "            print(\"%s of %s rows\" % (i, datalength))\n",
    "        \n",
    "        ## go through current row\n",
    "        comments = row[1].posts\n",
    "        ## remove urls, after % is ascii charcters so 2 bits\n",
    "        removeurls = re.sub('(http|https|ftp)://(?:[A-Za-z]|[0-9]|[$-_&@+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', comments)\n",
    "        ## remove non words\n",
    "        removenonwords = re.sub('[^A-Za-z]', ' ', removeurls)\n",
    "        removeextraspace = re.sub('  +',' ', removenonwords)\n",
    "        ## make lowercase \n",
    "        changed = removeextraspace.lower()\n",
    "        \n",
    "        #lemmatize basedoff of includiong stopwords or not \n",
    "        if remove_stopwords:\n",
    "            changed = \" \".join([lemmatizer.lemmatize(w) for w in changed.split(' ') if w not in stopWOrds])\n",
    "        else:\n",
    "            ## lemmatize\n",
    "            changed = \" \".join([lemmatizer.lemmatize(w) for w in changed.split(' ')])\n",
    "        \n",
    "        ## remove mbti types\n",
    "        if remove_mbti:\n",
    "            for m in mbti_types_list:\n",
    "                changed = changed.replace(m,\"\")\n",
    "                \n",
    "        ## make mbti typesbinary vectors\n",
    "        bin_vecs = mbti_to_bin_vec(row[1].type)\n",
    "        personalities.append(bin_vecs)\n",
    "        posts.append(changed)\n",
    "        \n",
    "    posts = np.array(posts)\n",
    "    personalities = np.array(personalities)\n",
    "    return posts, personalities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n",
      "8675 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "posts, personalities = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer...\n",
      "tf-idf...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "count_vec = CountVectorizer(stop_words=\"english\",\n",
    "                            analyzer = \"word\",\n",
    "                            ngram_range=(1,1),\n",
    "                            max_df=0.9,\n",
    "                            min_df=0.1,\n",
    "                            max_features=None)\n",
    "\n",
    "\n",
    "count_train = count_vec.fit(posts)\n",
    "print(\"CountVectorizer...\")\n",
    "# should create and return a count-vectorized output of docs\n",
    "posts_count = count_vec.fit_transform(posts)\n",
    "\n",
    "\n",
    "tfidf_izer = TfidfTransformer()\n",
    "\n",
    "print(\"tf-idf...\")\n",
    "posts_tfidf =  tfidf_izer.fit_transform(posts_count).toarray()                           \n",
    "                             \n",
    "#vectorizer = TfidfVectorizer()\n",
    "#vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "#feature_names = vectorizer.get_feature_names()\n",
    "#dense = vectors.todense()\n",
    "#denselist = dense.tolist()\n",
    "#df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) - Extroversion (E)\n",
      "NS: Intuition (N) - Sensing (S)\n",
      "FT: Feeling(F) - Thinking (T)\n",
      "JP: Judging (J) - Percieving (P)\n"
     ]
    }
   ],
   "source": [
    "# the MBTI type indicators\n",
    "mbti_type_ind = [\"IE: Introversion (I) - Extroversion (E)\",\n",
    "                \"NS: Intuition (N) - Sensing (S)\",\n",
    "                \"FT: Feeling(F) - Thinking (T)\",\n",
    "                \"JP: Judging (J) - Percieving (P)\"]\n",
    "\n",
    "for m in range(len(mbti_type_ind)):\n",
    "    print(mbti_type_ind[m])\n",
    "    \n",
    "mbti1row = bin_vec_to_mbti(personalities[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) - Extroversion (E) ...\n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ...\n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ...\n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ...\n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "X = posts_tfidf\n",
    "\n",
    "#we need to train each of the mbti type indicators\n",
    "for m in range(len(mbti_type_ind)):\n",
    "    print(\"%s ...\" % mbti_type_ind[m])\n",
    "    \n",
    "    #for each type indicator we train a different Y\n",
    "    Y = personalities[:,m]\n",
    "    \n",
    "    # next we split the data into train and test sets\n",
    "    #unsure how to choose the random seed or test size\n",
    "    seed = 50\n",
    "    test_size = 0.50\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    #fit model on the training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "        \n",
    "    #make predictions for the test data\n",
    "    Y_prediction = model.predict(X_test)\n",
    "    predictions = [round(value) for value in Y_prediction]\n",
    "    \n",
    "    #check the accuracy\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 1, 'colsample_bylevel': 1, 'booster': 'gbtree', 'scale_pos_weight': 1, 'reg_alpha': 0, 'n_estimators': 100, 'silent': 1, 'seed': 0, 'missing': None, 'reg_lambda': 1, 'base_score': 0.5, 'nthread': 1, 'gamma': 0, 'colsample_bytree': 1, 'max_delta_step': 0, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "#some of the xgbparams we may look at \n",
    "default_get_xgb_params = model.get_xgb_params()\n",
    "print(default_get_xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start iuth using the parameters:\n",
    "- n_estimators: number of trees to build\n",
    "- max_depth: the maximum. depth of a tree, \n",
    "    - the larger the value the more complex the model is and more likely to overfit \n",
    "- nthread: the number of parallel threads running XGBoost\n",
    "- learning_rate (also known as eta): step size shrinkage, in order to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.78%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.68%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.44%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 64.62%\n"
     ]
    }
   ],
   "source": [
    "#set up some of the parameters for xgboost\n",
    "param={}\n",
    "\n",
    "#good ones to choose initially would be n_estimators, max_depth, n_thread, learning_rate\n",
    "param['n_estimators']=100\n",
    "#max_depth default is 6 \n",
    "param['max_depth']=2\n",
    "#number of parallel threads running XGBoost\n",
    "param['nthread']=8\n",
    "#learning rate default is 0.3\n",
    "param['learning_rate']=0.2\n",
    "\n",
    "#training the MBTI type indicators individually\n",
    "for m in range(len(mbti_type_ind)):\n",
    "    print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "    \n",
    "    Y= personalities[:,m]\n",
    "    \n",
    "    #split into train and test sets\n",
    "    seed = 50\n",
    "    test_size = 0.50\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    #fit model on the training data\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, Y_train)\n",
    "        \n",
    "    #make predictions for the test data\n",
    "    Y_prediction = model.predict(X_test)\n",
    "    predictions = [round(value) for value in Y_prediction]\n",
    "    \n",
    "    #check the accuracy\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempts of parameter tuning did little with the accuracy not improving at all, with the rough estimates of accuracy still being:\n",
    "- IE: 77%\n",
    "- NS: 86%\n",
    "- FT: 73%\n",
    "- JP: 65%\n",
    "note: played around with the parameter, including setting n_estimates to 50 instead of 100, which reduced our accuracy by around 1% for each mbti type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also try:\n",
    "- subsample: ratio of the training istances, reducing means that less is sampled of the training data before growing trees\n",
    "- colsample_bytree: percentage of features used per tree. The higher it is the more likely it will lead to overfitting\n",
    "- num_parallel_tree: number of parallel trees constructed during each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing around with parameters\n",
    "We are going through testing different parameters to see which result in the best improvement of accuracy.\n",
    "The accuracy before parameters were added, for test_set=0.5 (one run):\n",
    "- IE: 77.41%\n",
    "- NS: 86.58%\n",
    "- TF: 73.28%\n",
    "- JP: 65.35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.57%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.08%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.31%\n",
      "max_depth\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.57%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.49%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.72%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.21%\n",
      "nthread\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "learning_rate\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.32%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.33%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.91%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.95%\n",
      "subsample\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.55%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.61%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.72%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.86%\n",
      "colsample_bytree\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.69%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.70%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.44%\n",
      "num_parallel_tree\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "num_feature\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "seed\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "silent\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "max_delta_step\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "scale_pos_weight\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 75.86%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.26%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 68.42%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 63.12%\n",
      "reg_lambda\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.80%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.56%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.65%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.05%\n",
      "colsample_bylevel\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.57%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.35%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.31%\n",
      "objective\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "base_score\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.22%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.17%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.56%\n",
      "min_child_weight\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.59%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.61%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.86%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 64.78%\n",
      "reg_alpha\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.69%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.61%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.72%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.15%\n",
      "gamma\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.69%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.61%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.37%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.63%\n"
     ]
    }
   ],
   "source": [
    "param={}\n",
    "\n",
    "\n",
    "param_list=['n_estimators', 'max_depth', 'nthread', 'learning_rate', 'subsample',\n",
    "          'colsample_bytree', 'num_parallel_tree', 'num_feature', 'seed',\n",
    "          'silent', 'max_delta_step', 'scale_pos_weight',\n",
    "          'reg_lambda', 'colsample_bylevel', 'objective', \n",
    "          'base_score', 'min_child_weight', 'reg_alpha', 'gamma']\n",
    "\n",
    "#previous value was 100\n",
    "param['n_estimators']=90 #no real change when set to 100\n",
    "\n",
    "#max_depth default is 6 , was set at 3\n",
    "param['max_depth']=4  #not a big improvement\n",
    "\n",
    "#number of parallel threads running XGBoost, was set at 1\n",
    "param['nthread']=2 #no real change\n",
    "\n",
    "#learning rate default is 0.3, was set at 0.1\n",
    "param['learning_rate']=0.2 #slight changes\n",
    "\n",
    "#default is 1\n",
    "param['subsample']=0.75 #a bit of change\n",
    "\n",
    "#default is 1\n",
    "param['colsample_bytree']= 0.75 #slight changes\n",
    "\n",
    "#default is 1\n",
    "param['num_parallel_tree'] = 2 #no real change\n",
    "\n",
    "#set automatically by XGBoost\n",
    "param['num_feature']= 25 #no real change\n",
    "\n",
    "#default 0\n",
    "param['seed']= 21\n",
    "\n",
    "#belongs to verbosity?\n",
    "param['silent']= 2\n",
    "\n",
    "#default 0\n",
    "param['max_delta_step']= 2\n",
    "\n",
    "#set at None\n",
    "#param['missing']=\n",
    "\n",
    "#set at 1\n",
    "param['scale_pos_weight']= 2\n",
    "\n",
    "#default is gbtree\n",
    "#param['booster'] = \"dgblinear\" #no real changes with gblinear or dart\n",
    "\n",
    "#default is 1\n",
    "param['reg_lambda']= 2\n",
    "\n",
    "#default is 1\n",
    "param['colsample_bylevel']=0.5\n",
    "\n",
    "#set at \"binary:logistoc\"\n",
    "param['objective']= \"reg:logistic\"\n",
    "\n",
    "#set at 0.5\n",
    "param['base_score']= 0.7\n",
    "\n",
    "#default 1\n",
    "param['min_child_weight']= 2\n",
    "\n",
    "#set at 0\n",
    "param['reg_alpha']= 1\n",
    "\n",
    "#default is 0\n",
    "param['gamma']= 1\n",
    "\n",
    "\n",
    "#go throguh all params\n",
    "for p in range(len(param_list)):\n",
    "    params={}  \n",
    "    params[param_list[p]] = param[param_list[p]]\n",
    "    print(param_list[p])\n",
    "    #training the MBTI type indicators individually\n",
    "    for m in range(len(mbti_type_ind)):\n",
    "        print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "\n",
    "        Y= personalities[:,m]\n",
    "\n",
    "        #split into train and test sets\n",
    "        seed = 50\n",
    "        test_size = 0.50\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "        #fit model on the training data\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        #make predictions for the test data\n",
    "        Y_prediction = model.predict(X_test)\n",
    "        predictions = [round(value) for value in Y_prediction]\n",
    "\n",
    "        #check the accuracy\n",
    "        accuracy = accuracy_score(Y_test, predictions)\n",
    "        print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.57%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.08%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.31%\n",
    "\n",
    "max_depth\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.57%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.49%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.72%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.21%\n",
    "\n",
    "nthread\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "learning_rate\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.32%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.33%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.91%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.95%\n",
    "\n",
    "subsample\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.55%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.61%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.72%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.86%\n",
    "\n",
    "colsample_bytree\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.69%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.70%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.44%\n",
    "\n",
    "num_parallel_tree\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "num_feature\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "seed\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    "FT: Feeling(F) - Thinking (T) ... \n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    "JP: Judging (J) - Percieving (P) ... \n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "silent\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    "FT: Feeling(F) - Thinking (T) ... \n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    "JP: Judging (J) - Percieving (P) ... \n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "max_delta_step\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    "FT: Feeling(F) - Thinking (T) ... \n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    "JP: Judging (J) - Percieving (P) ... \n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "scale_pos_weight\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 75.86%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.26%\n",
    "FT: Feeling(F) - Thinking (T) ... \n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 68.42%\n",
    "JP: Judging (J) - Percieving (P) ... \n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 63.12%\n",
    "\n",
    "reg_lambda\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.80%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.56%\n",
    "FT: Feeling(F) - Thinking (T) ... \n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.65%\n",
    "JP: Judging (J) - Percieving (P) ... \n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.05%\n",
    "\n",
    "colsample_bylevel\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.57%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
    "FT: Feeling(F) - Thinking (T) ... \n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.35%\n",
    "JP: Judging (J) - Percieving (P) ... \n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.31%\n",
    "\n",
    "objective\n",
    "IE: Introversion (I) - Extroversion (E) ... \n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    "NS: Intuition (N) - Sensing (S) ... \n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "base_score\n",
    " - IE: 77.22% \n",
    " - NS: 86.63%\n",
    " - FT: 73.17%\n",
    " - JP: 65.56%\n",
    "\n",
    "min_child_weight\n",
    " - IE: 77.59%\n",
    " - NS: 86.61%\n",
    " - FT: 73.86%\n",
    " - JP: 64.78%\n",
    "\n",
    "reg_alpha\n",
    " - IE: 77.69%\n",
    " - NS: 86.61%\n",
    " - FT: 73.72% \n",
    " - JP: 65.15%\n",
    "\n",
    "gamma\n",
    " - IE: 77.69%\n",
    " - NS: 86.61%\n",
    " - FT: 73.37%\n",
    " - JP: 65.63%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators = 90\n",
    " - IE: 77.57% \n",
    " - NS: 86.63%\n",
    " - FT: 73.08% \n",
    " - JP: 65.31%\n",
    " \n",
    "max_depth = 4\n",
    " - IE: 77.50%\n",
    " - NS: 86.51%\n",
    " - FT: 73.58%\n",
    " - JP: 65.31%\n",
    " \n",
    "nthread = 2\n",
    " - IE: 77.50%\n",
    " - NS: 86.51%\n",
    " - FT: 73.58%\n",
    " - JP: 65.31%\n",
    " \n",
    "learning_rate = 0.2\n",
    " - IE: 76.88%\n",
    " - NS: 86.17% \n",
    " - FT: 73.03%\n",
    " - JP: 64.08%\n",
    " \n",
    "subsample = 0.75\n",
    " - IE: 76.23%\n",
    " - NS: 86.10%\n",
    " - FT: 74.00%\n",
    " - JP: 64.25%\n",
    " \n",
    "colsample_bytree = 0.75\n",
    " - IE: 77.13%\n",
    " - NS: 86.10%\n",
    " - FT: 73.74%\n",
    " - JP: 62.93%\n",
    " \n",
    "num_parallel_tree = 2\n",
    " - IE: 77.48%\n",
    " - NS: 86.40%\n",
    " - FT: 74.69%\n",
    " - JP: 64.75%\n",
    " \n",
    "num_feature = 25\n",
    " - IE: 77.48%\n",
    " - NS: 86.40%\n",
    " - FT: 74.69%\n",
    " - JP: 64.75%\n",
    " \n",
    "seed = 21\n",
    " - IE: 77.06%\n",
    " - NS: 86.24%\n",
    " - FT: 75.20%\n",
    " - JP: 63.95%\n",
    " \n",
    "silent = 2\n",
    " - IE: 77.06%\n",
    " - NS: 86.24%\n",
    " - FT: 75.20%\n",
    " - JP: 63.95%\n",
    " \n",
    "max_delta_step = 2\n",
    " - IE: 77.06%\n",
    " - NS: 86.24%\n",
    " - FT: 75.20%\n",
    " - JP: 63.95%\n",
    " \n",
    "scale_pos_weight = 2\n",
    " - IE: 75.70% \n",
    " - NS: 85.82%\n",
    " - FT: 72.01%\n",
    " - JP: 65.28%\n",
    " \n",
    "reg_lambda = 2\n",
    " - IE: 75.68% \n",
    " - NS: 85.82%\n",
    " - FT: 73.03%\n",
    " - JP: 63.99%\n",
    " \n",
    "colsample_bylevel = 0.5\n",
    " - IE: 76.56%\n",
    " - NS: 85.34%\n",
    " - FT: 72.61%\n",
    " - JP: 64.04%\n",
    " \n",
    "objective = \"reg:logistic\"\n",
    " - IE: 76.56%\n",
    " - NS: 85.34%\n",
    " - FT: 72.61%\n",
    " - JP: 64.04%\n",
    " \n",
    "base_score = 0.7\n",
    " - IE: 76.33%\n",
    " - NS: 85.43%\n",
    " - FT: 72.57%\n",
    " - JP: 65.05%\n",
    " \n",
    "min_child_weight = 2\n",
    " - IE: 76.03%\n",
    " - NS: 85.34%\n",
    " - FT: 72.25%\n",
    " - JP: 63.53%\n",
    " \n",
    "reg_alpha = 1\n",
    " - IE: 75.96%\n",
    " - NS: 85.36%\n",
    " - FT: 72.22%\n",
    " - JP: 64.59%\n",
    " \n",
    "gamma = 1\n",
    " - IE: 76.35%\n",
    " - NS: 85.52%\n",
    " - FT: 72.43% \n",
    " - JP: 63.95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feature = 0\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "num_feature = 1\n",
      "IE: Introversion (I) - Extroversion (E) ... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d8a5aa45166c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m#fit model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#make predictions for the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    545\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1021\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param={}\n",
    "\n",
    "param_list = ['num_feature', \n",
    "              'seed', 'silent', 'max_delta_step', 'scale_pos_weight', \n",
    "              'reg_lambda', 'min_child_weight', 'reg_alpha', 'gamma']\n",
    "random_num_list=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "\n",
    "#max_depth default is 6 , was set at 3\n",
    "param['max_depth']=4  #not a big improvement\n",
    "\n",
    "#number of parallel threads running XGBoost, was set at 1\n",
    "param['nthread']=2 #no real change\n",
    "\n",
    "#default is 1\n",
    "param['num_parallel_tree'] = 2 #no real change\n",
    "\n",
    "#set automatically by XGBoost\n",
    "param['num_feature']= 25 #no real change\n",
    "\n",
    "#default 0\n",
    "param['seed']= 21\n",
    "\n",
    "#belongs to verbosity?\n",
    "param['silent']= 2\n",
    "\n",
    "#default 0\n",
    "param['max_delta_step']= 2\n",
    "\n",
    "\n",
    "#set at 1\n",
    "param['scale_pos_weight']= 2\n",
    "\n",
    "#default is 1\n",
    "param['reg_lambda']= 2\n",
    "\n",
    "#default 1\n",
    "param['min_child_weight']= 2\n",
    "\n",
    "#set at 0\n",
    "param['reg_alpha']= 1\n",
    "\n",
    "#default is 0\n",
    "param['gamma']= 1\n",
    "\n",
    "#go throguh all params\n",
    "\n",
    "for p in range(len(param_list)):\n",
    "    for r in range(len(random_num_list)):\n",
    "        params={}  \n",
    "        params[param_list[p]] = random_num_list[r]\n",
    "        print('%s = %s' % (param_list[p], random_num_list[r]))\n",
    "        #training the MBTI type indicators individually\n",
    "        for m in range(len(mbti_type_ind)):\n",
    "            print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "\n",
    "            Y= personalities[:,m]\n",
    "\n",
    "            #split into train and test sets\n",
    "            seed = 50\n",
    "            test_size = 0.50\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "            #fit model on the training data\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            #make predictions for the test data\n",
    "            Y_prediction = model.predict(X_test)\n",
    "            predictions = [round(value) for value in Y_prediction]\n",
    "\n",
    "            #check the accuracy\n",
    "            accuracy = accuracy_score(Y_test, predictions)\n",
    "            print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_feature = 0 \n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 1\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 2\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 3\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 4\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 5 \n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28% \n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 6\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28% \n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 7\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 8\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 9\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "num_feature = 10 \n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "\n",
    "seed = 0\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 1 \n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 2\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 3\n",
    " - IE: 77.41%\n",
    " - NS: 86.58% \n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 4\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 5\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 6\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28% \n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 7\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 8\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 9\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "seed = 10\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "\n",
    "silent = 1\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 2\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28% \n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 3\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 4\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 5\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 6\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 7\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 8\n",
    " - IE: 77.41%\n",
    " - NS: 86.58% \n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 9\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "silent = 10 \n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "\n",
    "max_delta_step = 0 \n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 1\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 2\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 3\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 4\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 5\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 6\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 7\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 8\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 9\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_delta_step = 10\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "\n",
    "scale_pos_weight = 0\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 46.59%\n",
    " - JP: 39.65%\n",
    "\n",
    "scale_pos_weight = 1\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "scale_pos_weight = 2\n",
    " - IE: 75.86%\n",
    " - NS: 86.26%\n",
    " - FT: 68.42%\n",
    " - JP: 63.12%\n",
    "\n",
    "scale_pos_weight = 3\n",
    " - IE: 71.35%\n",
    " - NS: 84.39%\n",
    " - FT: 64.59%\n",
    " - JP: 61.78%\n",
    "\n",
    "scale_pos_weight = 4\n",
    " - IE: 65.35%\n",
    " - NS: 81.65%\n",
    " - FT: 62.03%\n",
    " - JP: 61.27%\n",
    "\n",
    "scale_pos_weight = 5\n",
    " - IE: 58.92%\n",
    " - NS: 78.28%\n",
    " - FT: 61.02%\n",
    " - JP: 61.02%\n",
    "\n",
    "scale_pos_weight = 6\n",
    " - IE: 54.08%\n",
    " - NS: 74.83%\n",
    " - FT: 59.61%\n",
    " - JP: 61.04%\n",
    "\n",
    "scale_pos_weight = 7\n",
    " - IE: 48.76%\n",
    " - NS: 71.30%\n",
    " - FT: 59.15%\n",
    " - JP: 60.88%\n",
    "\n",
    "scale_pos_weight = 8\n",
    " - IE: 45.78%\n",
    " - NS: 67.96%\n",
    " - FT: 57.98% \n",
    " - JP: 60.79%\n",
    "\n",
    "scale_pos_weight = 9\n",
    " - IE: 43.68%\n",
    " - NS: 65.12%\n",
    " - FT: 57.61%\n",
    " - JP: 60.81%\n",
    "\n",
    "scale_pos_weight = 10\n",
    " - IE: 41.68%\n",
    " - NS: 62.61%\n",
    " - FT: 57.12%\n",
    " - JP: 60.81%\n",
    "\n",
    "\n",
    "reg_lambda = 0\n",
    " - IE: 42.21%\n",
    " - NS: 61.71%\n",
    " - FT: 57.93%\n",
    " - JP: 61.07%\n",
    "\n",
    "reg_lambda = 1\n",
    " - IE: 41.68%\n",
    " - NS: 62.61%\n",
    " - FT: 57.12%\n",
    " - JP: 60.81%\n",
    "\n",
    "reg_lambda = 2\n",
    " - IE: 41.56%\n",
    " - NS: 62.22%\n",
    " - FT: 57.22%\n",
    " - JP: 60.63%\n",
    "\n",
    "reg_lambda = 3\n",
    " - IE: 41.70%\n",
    " - NS: 62.17%\n",
    " - FT: 57.19%\n",
    " - JP: 60.72%\n",
    "\n",
    "reg_lambda = 4\n",
    " - IE: 41.01%\n",
    " - NS: 62.77%\n",
    " - FT: 56.92%\n",
    " - JP: 60.74%\n",
    "\n",
    "reg_lambda = 5\n",
    " - IE: 40.55%\n",
    " - NS: 62.24%\n",
    " - FT: 56.80%\n",
    " - JP: 60.63%\n",
    "\n",
    "reg_lambda = 6\n",
    " - IE: 40.57%\n",
    " - NS: 61.96%\n",
    " - FT: 56.96%\n",
    " - JP: 60.63%\n",
    "\n",
    "reg_lambda = 7\n",
    " - IE: 40.27%\n",
    " - NS: 62.06%\n",
    " - FT: 56.59%\n",
    " - JP: 60.65%\n",
    "\n",
    "reg_lambda = 8\n",
    " - IE: 40.23% \n",
    " - NS: 61.00%\n",
    " - FT: 56.55%\n",
    " - JP: 60.56%\n",
    "\n",
    "reg_lambda = 9\n",
    " - IE: 39.95%\n",
    " - NS: 61.71%\n",
    " - FT: 56.59%\n",
    " - JP: 60.60%\n",
    "\n",
    "reg_lambda = 10\n",
    " - IE: 39.35%\n",
    " - NS: 61.50%\n",
    " - FT: 56.39%\n",
    " - JP: 60.56%\n",
    "\n",
    "\n",
    "min_child_weight = 0\n",
    " - IE: 39.40%\n",
    " - NS: 62.17%\n",
    " - FT: 56.57%\n",
    " - JP: 60.56%\n",
    "\n",
    "min_child_weight = 1\n",
    " - IE: 39.35% \n",
    " - NS: 61.50%\n",
    " - FT: 56.39%\n",
    " - JP: 60.56%\n",
    "\n",
    "min_child_weight = 2\n",
    " - IE: 39.67%\n",
    " - NS: 61.57%\n",
    " - FT: 56.75% \n",
    " - JP: 60.60%\n",
    "\n",
    "min_child_weight = 3\n",
    " - IE: 39.51%\n",
    " - NS: 60.86%\n",
    " - FT: 56.45%\n",
    " - JP: 60.58%\n",
    "\n",
    "min_child_weight = 4\n",
    " - IE: 39.81%\n",
    " - NS: 61.62%\n",
    " - FT: 56.13%\n",
    " - JP: 60.56%\n",
    "\n",
    "min_child_weight = 5\n",
    " - IE: 40.66%\n",
    " - NS: 61.25%\n",
    " - FT: 56.36%\n",
    " - JP: 60.63%\n",
    "\n",
    "min_child_weight = 6\n",
    " - IE: 39.47%\n",
    " - NS: 61.76%\n",
    " - FT: 56.18%\n",
    " - JP: 60.60%\n",
    "\n",
    "min_child_weight = 7\n",
    " - IE: 39.93%\n",
    " - NS: 60.63%\n",
    " - FT: 56.20%\n",
    " - JP: 60.56%\n",
    "\n",
    "min_child_weight = 8\n",
    " - IE: 39.40%\n",
    " - NS: 60.77%\n",
    " - FT: 56.32%\n",
    " - JP: 60.58%\n",
    "\n",
    "min_child_weight = 9\n",
    " - IE: 38.96%\n",
    " - NS: 60.88%\n",
    " - FT: 56.34%\n",
    " - JP: 60.63%\n",
    "\n",
    "min_child_weight = 10\n",
    " - IE: 39.44%\n",
    " - NS: 61.23%\n",
    " - FT: 56.11%\n",
    " - JP: 60.60%\n",
    "\n",
    "\n",
    "reg_alpha = 0\n",
    " - IE: 39.44%\n",
    " - NS: 61.23%\n",
    " - FT: 56.11% \n",
    " - JP: 60.60%\n",
    "\n",
    "reg_alpha = 1\n",
    " - IE: 39.44%\n",
    " - NS: 61.11%\n",
    " - FT: 56.27%\n",
    " - JP: 60.67%\n",
    "\n",
    "reg_alpha = 2\n",
    " - IE: 39.42%\n",
    " - NS: 61.48%\n",
    " - FT: 56.11%\n",
    " - JP: 60.60%\n",
    "\n",
    "reg_alpha = 3\n",
    " - IE: 39.17%\n",
    " - NS: 61.25%\n",
    " - FT: 56.06% \n",
    " - JP: 60.56%\n",
    "\n",
    "reg_alpha = 4\n",
    " - IE: 39.58%\n",
    " - NS: 61.11%\n",
    " - FT: 55.92%\n",
    " - JP: 60.58%\n",
    "\n",
    "reg_alpha = 5\n",
    " - IE: 38.93%\n",
    " - NS: 61.00%\n",
    " - FT: 55.99%\n",
    " - JP: 60.63%\n",
    "\n",
    "reg_alpha = 6\n",
    " - IE: 38.13%\n",
    " - NS: 60.90%\n",
    " - FT: 55.81%\n",
    " - JP: 60.56%\n",
    "\n",
    "reg_alpha = 7\n",
    " - IE: 38.38%\n",
    " - NS: 60.83%\n",
    " - FT: 55.76%\n",
    " - JP: 60.67%\n",
    "\n",
    "reg_alpha = 8\n",
    " - IE: 38.66%\n",
    " - NS: 60.93%\n",
    " - FT: 55.49%\n",
    " - JP: 60.63%\n",
    "\n",
    "reg_alpha = 9\n",
    " - IE: 37.30%\n",
    " - NS: 60.63% \n",
    " - FT: 55.49%\n",
    " - JP: 60.63%\n",
    "\n",
    "reg_alpha = 10\n",
    " - IE: 38.61%\n",
    " - NS: 60.33% \n",
    " - FT: 55.56%\n",
    " - JP: 60.60%\n",
    "\n",
    "\n",
    "gamma = 0\n",
    " - IE: 38.61%\n",
    " - NS: 60.33%\n",
    " - FT: 55.56%\n",
    " - JP: 60.60%\n",
    "\n",
    "gamma = 1\n",
    " - IE: 37.97%\n",
    " - NS: 61.39%\n",
    " - FT: 55.56%\n",
    " - JP: 60.60%\n",
    "\n",
    "gamma = 2\n",
    " - IE: 38.43%\n",
    " - NS: 60.95%\n",
    " - FT: 55.58%\n",
    " - JP: 60.67%\n",
    "\n",
    "gamma = 3\n",
    " - IE: 37.71%\n",
    " - NS: 60.14%\n",
    " - FT: 55.58%\n",
    " - JP: 60.65%\n",
    "\n",
    "gamma = 4\n",
    " - IE: 38.01%\n",
    " - NS: 60.63%\n",
    " - FT: 55.37%\n",
    " - JP: 60.60%\n",
    "\n",
    "gamma = 5\n",
    " - IE: 38.06%\n",
    " - NS: 60.65%\n",
    " - FT: 55.46%\n",
    " - JP: 60.60%\n",
    "\n",
    "gamma = 6\n",
    " - IE: 38.01%\n",
    " - NS: 60.90%\n",
    " - FT: 55.60%\n",
    " - JP: 60.65%\n",
    "\n",
    "gamma = 7\n",
    " - IE: 37.57%\n",
    " - NS: 60.93%\n",
    " - FT: 55.44%\n",
    " - JP: 60.60%\n",
    "\n",
    "gamma = 8\n",
    " - IE: 38.04%\n",
    " - NS: 60.86% \n",
    " - FT: 55.49% \n",
    " - JP: 60.56%\n",
    "\n",
    "gamma = 9 \n",
    " - IE: 37.57% \n",
    " - NS: 60.60%\n",
    " - FT: 55.49%\n",
    " - JP: 60.58%\n",
    "\n",
    "gamma = 10\n",
    " - IE: 37.46%\n",
    " - NS: 60.72%\n",
    " - FT: 55.37%\n",
    " - JP: 60.58%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth = 0\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 53.41%\n",
    " - JP: 60.35%\n",
    "\n",
    "max_depth = 1\n",
    " - IE: 77.09%\n",
    " - NS: 86.63%\n",
    " - FT: 70.91% \n",
    " - JP: 64.64%\n",
    "\n",
    "max_depth = 2 \n",
    " - IE: 77.39%\n",
    " - NS: 86.63%\n",
    " - FT: 72.18%\n",
    " - JP: 65.21%\n",
    "\n",
    "max_depth = 3\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "max_depth = 4\n",
    " - IE: 77.57%\n",
    " - NS: 86.49%\n",
    " - FT: 73.72%\n",
    " - JP: 65.21%\n",
    "\n",
    "max_depth = 5\n",
    " - IE: 77.29%\n",
    " - NS: 86.54%\n",
    " - FT: 73.44%\n",
    " - JP: 64.59%\n",
    "\n",
    "max_depth = 6\n",
    " - IE: 77.18%\n",
    " - NS: 86.45%\n",
    " - FT: 74.16%\n",
    " - JP: 65.17%\n",
    "\n",
    "max_depth = 7\n",
    " - IE: 77.39%\n",
    " - NS: 86.54%\n",
    " - FT: 73.97%\n",
    " - JP: 64.59%\n",
    "\n",
    "max_depth = 8\n",
    " - IE: 77.09%\n",
    " - NS: 86.49%\n",
    " - FT: 73.61%\n",
    " - JP: 64.91%\n",
    "\n",
    "max_depth = 9\n",
    " - IE: 77.13%\n",
    " - NS: 86.51%\n",
    " - FT: 72.87%\n",
    " - JP: 64.87%\n",
    "\n",
    "max_depth = 10\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "\n",
    "nthread = 0\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 1\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 2\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 3\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 4\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 5\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 6\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 7\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 8\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86% \n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 9\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86% \n",
    " - JP: 64.73%\n",
    "\n",
    "nthread = 10\n",
    " - IE: 77.27%\n",
    " - NS: 86.58%\n",
    " - FT: 73.86%\n",
    " - JP: 64.73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param={}\n",
    "\n",
    "param_list = ['learning_rate', 'subsample', 'colsample_bytree', \n",
    "              'colsample_bylevel', 'base_score']\n",
    "random_num_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "\n",
    "#learning rate default is 0.3, was set at 0.1\n",
    "param['learning_rate']=0.2 #slight changes\n",
    "\n",
    "#default is 1\n",
    "param['subsample']=0.75 #a bit of change\n",
    "\n",
    "#default is 1\n",
    "param['colsample_bytree']= 0.75 #slight changes\n",
    "\n",
    "#default is 1\n",
    "param['colsample_bylevel']=0.5\n",
    "\n",
    "#set at 0.5\n",
    "param['base_score']= 0.7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#go throguh all params\n",
    "for p in range(len(param_list)):\n",
    "    for r in range(len(random_num_list)):\n",
    "        params={}  \n",
    "        params[param_list[p]] = random_num_list[r]\n",
    "        print('%s = %s' % (param_list[p], random_num_list[r]))\n",
    "        #training the MBTI type indicators individually\n",
    "        for m in range(len(mbti_type_ind)):\n",
    "            print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "\n",
    "            Y= personalities[:,m]\n",
    "\n",
    "            #split into train and test sets\n",
    "            seed = 50\n",
    "            test_size = 0.50\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "            #fit model on the training data\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            #make predictions for the test data\n",
    "            Y_prediction = model.predict(X_test)\n",
    "            predictions = [round(value) for value in Y_prediction]\n",
    "\n",
    "            #check the accuracy\n",
    "            accuracy = accuracy_score(Y_test, predictions)\n",
    "            print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate = 0.1\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    " \n",
    "learning_rate = 0.2\n",
    " - IE: 77.32%\n",
    " - NS: 86.33%\n",
    " - FT: 73.91%\n",
    " - JP: 65.95%\n",
    "\n",
    "learning_rate = 0.3\n",
    " - IE: 77.20%\n",
    " - NS: 85.82%\n",
    " - FT: 74.14%\n",
    " - JP: 63.42%\n",
    "\n",
    "learning_rate = 0.4\n",
    " - IE: 76.37%\n",
    " - NS: 85.27%\n",
    " - FT: 73.61%\n",
    " - JP: 63.21%\n",
    "\n",
    "learning_rate = 0.5\n",
    " - IE: 75.63% \n",
    " - NS: 84.44%\n",
    " - FT: 72.50%\n",
    " - JP: 62.03%\n",
    "\n",
    "learning_rate = 0.6 \n",
    " - IE: 74.25%\n",
    " - NS: 84.97%\n",
    " - FT: 71.12% \n",
    " - JP: 62.13%\n",
    "\n",
    "learning_rate = 0.7\n",
    " - IE: 73.81%\n",
    " - NS: 84.07% \n",
    " - FT: 71.12%\n",
    " - JP: 61.66%\n",
    "\n",
    "learning_rate = 0.8\n",
    " - IE: 72.75%\n",
    " - NS: 83.82%\n",
    " - FT: 71.23%\n",
    " - JP: 60.33%\n",
    "\n",
    "learning_rate = 0.9\n",
    " - IE: 72.54%\n",
    " - NS: 83.47%\n",
    " - FT: 69.64% \n",
    " - JP: 60.00%\n",
    "\n",
    "learning_rate = 1\n",
    " - IE: 72.87%\n",
    " - NS: 83.06%\n",
    " - FT: 69.82%\n",
    " - JP: 60.63%\n",
    "\n",
    "\n",
    "subsample = 0.1\n",
    " - IE: 65.95%\n",
    " - NS: 76.88%\n",
    " - FT: 61.64%\n",
    " - JP: 56.62%\n",
    "\n",
    "subsample = 0.2\n",
    " - IE: 66.57%\n",
    " - NS: 75.33%\n",
    " - FT: 62.68%\n",
    " - JP: 57.77%\n",
    "\n",
    "subsample = 0.3\n",
    " - IE: 68.12%\n",
    " - NS: 75.15%\n",
    " - FT: 62.84%\n",
    " - JP: 56.87%\n",
    "\n",
    "subsample = 0.4\n",
    " - IE: 66.76%\n",
    " - NS: 75.56%\n",
    " - FT: 64.41%\n",
    " - JP: 57.38%\n",
    "\n",
    "subsample = 0.5\n",
    " - IE: 66.87%\n",
    " - NS: 77.73%\n",
    " - FT: 66.18% \n",
    " - JP: 56.64%\n",
    "\n",
    "subsample = 0.6\n",
    " - IE: 70.29%\n",
    " - NS: 79.85%\n",
    " - FT: 67.73%\n",
    " - JP: 59.43%\n",
    "\n",
    "subsample = 0.7 \n",
    " - IE: 70.59%\n",
    " - NS: 80.71%\n",
    " - FT: 69.64%\n",
    " - JP: 57.98%\n",
    "\n",
    "subsample = 0.8\n",
    " - IE: 71.42%\n",
    " - NS: 81.49%\n",
    " - FT: 69.89%\n",
    " - JP: 59.89%\n",
    "\n",
    "subsample = 0.9\n",
    " - IE: 71.97%\n",
    " - NS: 82.09%\n",
    " - FT: 70.31%\n",
    " - JP: 60.88%\n",
    "\n",
    "subsample = 1\n",
    " - IE: 72.87%\n",
    " - NS: 83.06%\n",
    " - FT: 69.82%\n",
    " - JP: 60.63%\n",
    "\n",
    "\n",
    "colsample_bytree = 0.1\n",
    " - IE: 72.18%\n",
    " - NS: 82.62%\n",
    " - FT: 69.41%\n",
    " - JP: 59.29%\n",
    "\n",
    "colsample_bytree = 0.2\n",
    " - IE: 71.25%\n",
    " - NS: 82.39%\n",
    " - FT: 69.39% \n",
    " - JP: 60.24%\n",
    "\n",
    "colsample_bytree = 0.3\n",
    " - IE: 72.52%\n",
    " - NS: 82.32%\n",
    " - FT: 71.14%\n",
    " - JP: 61.30%\n",
    "\n",
    "colsample_bytree = 0.4\n",
    " - IE: 72.13%\n",
    " - NS: 82.90%\n",
    " - FT: 69.69% \n",
    " - JP: 61.46%\n",
    "\n",
    "colsample_bytree = 0.5\n",
    " - IE: 72.20%\n",
    " - NS: 82.80%\n",
    " - FT: 69.94%\n",
    " - JP: 59.20%\n",
    "\n",
    "colsample_bytree = 0.6\n",
    " - IE: 72.36%\n",
    " - NS: 82.76%\n",
    " - FT: 69.80%\n",
    " - JP: 60.35%\n",
    "\n",
    "colsample_bytree = 0.7\n",
    " - IE: 72.45%\n",
    " - NS: 82.80%\n",
    " - FT: 70.75%\n",
    " - JP: 59.75%\n",
    "\n",
    "colsample_bytree = 0.8\n",
    " - IE: 72.73%\n",
    " - NS: 83.31%\n",
    " - FT: 70.31%\n",
    " - JP: 59.94%\n",
    "\n",
    "colsample_bytree = 0.9\n",
    " - IE: 72.68%\n",
    " - NS: 82.78%\n",
    " - FT: 70.42%\n",
    " - JP: 59.89%\n",
    "\n",
    "colsample_bytree = 1\n",
    " - IE: 72.87%\n",
    " - NS: 83.06%\n",
    " - FT: 69.82%\n",
    " - JP: 60.63%\n",
    "\n",
    "\n",
    "colsample_bylevel = 0.1\n",
    " - IE: 72.41%\n",
    " - NS: 82.07%\n",
    " - FT: 70.59%\n",
    " - JP: 59.06%\n",
    "\n",
    "colsample_bylevel = 0.2\n",
    " - IE: 72.57%\n",
    " - NS: 82.64%\n",
    " - FT: 70.31%\n",
    " - JP: 59.27%\n",
    "\n",
    "colsample_bylevel = 0.3\n",
    " - IE: 72.36%\n",
    " - NS: 82.00%\n",
    " - FT: 70.26%\n",
    " - JP: 59.94%\n",
    "\n",
    "colsample_bylevel = 0.4\n",
    " - IE: 73.49%\n",
    " - NS: 82.73%\n",
    " - FT: 70.12%\n",
    " - JP: 60.42%\n",
    "\n",
    "colsample_bylevel = 0.5\n",
    " - IE: 72.11%\n",
    " - NS: 83.03%\n",
    " - FT: 70.17%\n",
    " - JP: 59.75%\n",
    "\n",
    "colsample_bylevel = 0.6\n",
    " - IE: 72.04%\n",
    " - NS: 82.25%\n",
    " - FT: 69.29%\n",
    " - JP: 59.73%\n",
    "\n",
    "colsample_bylevel = 0.7\n",
    " - IE: 72.87% \n",
    " - NS: 83.47%. \n",
    " - FT: 70.95%\n",
    " - JP: 61.96%\n",
    "\n",
    "colsample_bylevel = 0.8\n",
    " - IE: 72.50%\n",
    " - NS: 83.15%\n",
    " - FT: 69.69%\n",
    " - JP: 60.65%\n",
    "\n",
    "colsample_bylevel = 0.9\n",
    " - IE: 72.84%\n",
    " - NS: 83.45%\n",
    " - FT: 71.09%\n",
    " - JP: 60.26%\n",
    "\n",
    "colsample_bylevel = 1\n",
    " - IE: 72.87%\n",
    " - NS: 83.06%\n",
    " - FT: 69.82%\n",
    " - JP: 60.63%\n",
    "\n",
    "\n",
    "base_score = 0.1 \n",
    " - IE: 73.35%\n",
    " - NS: 82.39%\n",
    " - FT: 65.35%\n",
    " - JP: 39.70%\n",
    "\n",
    "base_score = 0.2\n",
    " - IE: 72.48%\n",
    " - NS: 83.36%\n",
    " - FT: 69.71%\n",
    " - JP: 60.00%\n",
    "\n",
    "base_score = 0.3 \n",
    " - IE: 72.04%\n",
    " - NS: 83.31%\n",
    " - FT: 70.45%\n",
    " - JP: 60.67%\n",
    "\n",
    "base_score = 0.4\n",
    " - IE: 72.61%\n",
    " - NS: 82.96%\n",
    " - FT: 70.45%\n",
    " - JP: 60.74%\n",
    "\n",
    "base_score = 0.5\n",
    " - IE: 72.87%\n",
    " - NS: 83.06%\n",
    " - FT: 69.82%\n",
    " - JP: 60.63%\n",
    "\n",
    "base_score = 0.6\n",
    " - IE: 72.61%\n",
    " - NS: 83.47%\n",
    " - FT: 69.73%\n",
    " - JP: 61.27%\n",
    "\n",
    "base_score = 0.7\n",
    " - IE: 72.71%\n",
    " - NS: 82.99%\n",
    " - FT: 71.62%\n",
    " - JP: 60.10%\n",
    "\n",
    "base_score = 0.8\n",
    " - IE: 72.11% \n",
    " - NS: 82.48%\n",
    " - FT: 70.59%\n",
    " - JP: 60.51%\n",
    "\n",
    "base_score = 0.9\n",
    " - IE: 29.23% \n",
    " - NS: 76.12%\n",
    " - FT: 60.33%\n",
    " - JP: 59.87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param_list=['n_estimators', 'num_feature', 'seed', 'gamma']\n",
    "\n",
    "random_num_list=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
    "\n",
    "#previous value was 100\n",
    "param['n_estimators']=90 #no real change when set to 100\n",
    "\n",
    "#set automatically by XGBoost\n",
    "param['num_feature']= 25 #no real change\n",
    "\n",
    "#default 0\n",
    "param['seed']= 21\n",
    "\n",
    "\n",
    "#default is 0\n",
    "param['gamma']= 1\n",
    "\n",
    "\n",
    "#go throguh all params\n",
    "for p in range(len(param_list)):\n",
    "    for r in range(len(random_num_list)):\n",
    "        params={}  \n",
    "        params[param_list[p]] = random_num_list[r]\n",
    "        print('%s = %s' % (param_list[p], random_num_list[r]))\n",
    "        #training the MBTI type indicators individually\n",
    "        for m in range(len(mbti_type_ind)):\n",
    "            print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "\n",
    "            Y= personalities[:,m]\n",
    "\n",
    "            #split into train and test sets\n",
    "            seed = 50\n",
    "            test_size = 0.50\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "            #fit model on the training data\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            #make predictions for the test data\n",
    "            Y_prediction = model.predict(X_test)\n",
    "            predictions = [round(value) for value in Y_prediction]\n",
    "\n",
    "            #check the accuracy\n",
    "            accuracy = accuracy_score(Y_test, predictions)\n",
    "            print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators = 10\n",
    " - IE: 77.22%\n",
    " - NS: 86.65%\n",
    " - FT: 66.62%\n",
    " - JP: 64.41%\n",
    "\n",
    "n_estimators = 20\n",
    " - IE: 77.09%\n",
    " - NS: 86.63%\n",
    " - FT: 69.13%\n",
    " - JP: 64.38%\n",
    "\n",
    "n_estimators = 30\n",
    " - IE: 77.13%\n",
    " - NS: 86.63%\n",
    " - FT: 70.52%\n",
    " - JP: 64.64%\n",
    "\n",
    "n_estimators = 40\n",
    " - IE: 77.18%\n",
    " - NS: 86.63%\n",
    " - FT: 71.21%\n",
    " - JP: 64.96%\n",
    "\n",
    "n_estimators = 50\n",
    " - IE: 77.36%\n",
    " - NS: 86.63%\n",
    " - FT: 71.92%\n",
    " - JP: 64.78%\n",
    "\n",
    "n_estimators = 60\n",
    " - IE: 77.48%\n",
    " - NS: 86.63%\n",
    " - FT: 72.13%\n",
    " - JP: 65.19%\n",
    "\n",
    "n_estimators = 70\n",
    " - IE: 77.50%\n",
    " - NS: 86.63%\n",
    " - FT: 72.80%\n",
    " - JP: 65.65%\n",
    "\n",
    "n_estimators = 80\n",
    " - IE: 77.32%\n",
    " - NS: 86.63%\n",
    " - FT: 73.03%\n",
    " - JP: 65.21%\n",
    "\n",
    "n_estimators = 90 \n",
    " - IE: 77.57%\n",
    " - NS: 86.63%\n",
    " - FT: 73.08%\n",
    " - JP: 65.31%\n",
    "\n",
    "n_estimators = 100\n",
    " - IE: 77.41%\n",
    " - NS: 86.58%\n",
    " - FT: 73.28%\n",
    " - JP: 65.35%\n",
    "\n",
    "n_estimators = 110\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "\n",
    "num_feature = 10 \n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 20\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 30\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 40\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 50\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 60\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 70\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 80\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 90\n",
    " - IE: 77.39% \n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 100\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "num_feature = 110\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "\n",
    "seed = 10\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 20\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 30\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 40\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 50\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 60\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 70\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 80\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 90\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 100\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "seed = 110\n",
    " - IE: 77.39%\n",
    " - NS: 86.58%\n",
    " - FT: 73.77%\n",
    " - JP: 65.03%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n",
    "gamma = 10\n",
    " - IE: 77.62%\n",
    " - NS: 86.63%\n",
    " - FT: 73.91%\n",
    " - JP: 64.96%\n",
    "\n",
    "gamma = 20\n",
    " - IE: 77.09%\n",
    " - NS: 86.63%\n",
    " - FT: 71.23%\n",
    " - JP: 64.43%\n",
    "\n",
    "gamma = 30\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 69.96%\n",
    " - JP: 64.27%\n",
    "\n",
    "gamma = 40\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 68.37% \n",
    " - JP: 61.27%\n",
    "\n",
    "gamma = 50\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 66.74%\n",
    " - JP: 60.35%\n",
    "\n",
    "gamma = 60\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 66.25%\n",
    " - JP: 60.35%\n",
    "\n",
    "gamma = 70\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 65.12%\n",
    " - JP: 60.35%\n",
    "\n",
    "gamma = 80\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 64.98%\n",
    " - JP: 60.35%\n",
    "\n",
    "gamma = 90\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 64.59% \n",
    " - JP: 60.35%\n",
    "\n",
    "gamma = 100\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 62.93%\n",
    " - JP: 60.35%\n",
    "\n",
    "gamma = 110\n",
    " - IE: 77.06%\n",
    " - NS: 86.63%\n",
    " - FT: 61.60%\n",
    " - JP: 60.35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster = gbtree\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
      "booster = gblinear\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.06%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 53.41%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 60.35%\n",
      "booster = dart\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
      "NS: Intuition (N) - Sensing (S) ... \n",
      " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
      "FT: Feeling(F) - Thinking (T) ... \n",
      " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
      "JP: Judging (J) - Percieving (P) ... \n",
      " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n"
     ]
    }
   ],
   "source": [
    "param={}\n",
    "\n",
    "param_list = ['booster']\n",
    "random_num_list=['gbtree', 'gblinear', 'dart']\n",
    "\n",
    "\n",
    "#default is gbtree\n",
    "param['booster'] = \"dgblinear\" #no real changes with gblinear or dart\n",
    "\n",
    "\n",
    "#go throguh all params\n",
    "params={}  \n",
    "for p in range(len(param_list)):\n",
    "    for r in range(len(random_num_list)):\n",
    "        params={}  \n",
    "        params[param_list[p]] = random_num_list[r]\n",
    "        print('%s = %s' % (param_list[p], random_num_list[r]))\n",
    "        #training the MBTI type indicators individually\n",
    "        for m in range(len(mbti_type_ind)):\n",
    "            print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "\n",
    "            Y= personalities[:,m]\n",
    "\n",
    "            #split into train and test sets\n",
    "            seed = 50\n",
    "            test_size = 0.50\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "            #fit model on the training data\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            #make predictions for the test data\n",
    "            Y_prediction = model.predict(X_test)\n",
    "            predictions = [round(value) for value in Y_prediction]\n",
    "\n",
    "            #check the accuracy\n",
    "            accuracy = accuracy_score(Y_test, predictions)\n",
    "            print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "booster = gbtree\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%\n",
    "\n",
    "booster = gblinear\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.06%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.63%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 53.41%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 60.35%\n",
    "\n",
    "booster = dart\n",
    " - IE: Introversion (I) - Extroversion (E) Accuracy: 77.41%\n",
    " - NS: Intuition (N) - Sensing (S) Accuracy: 86.58%\n",
    " - FT: Feeling(F) - Thinking (T) Accuracy: 73.28%\n",
    " - JP: Judging (J) - Percieving (P) Accuracy: 65.35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective = reg:squarederror\n",
      "IE: Introversion (I) - Extroversion (E) ... \n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: reg:linear\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: reg:logistic\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: binary:logistic\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: binary:logitraw\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: count:poisson\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: survival:cox\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: reg:gamma\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: reg:tweedie\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: rank:pairwise\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: rank:ndcg\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: rank:map\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: multi:softmax\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: multi:softprob\n",
      "[18:03:08] src/objective/objective.cc:21: Objective candidate: binary:hinge\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "b'[18:03:08] src/objective/objective.cc:23: Unknown objective function reg:squarederror\\n\\nStack trace returned 8 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x000000201a35e3bd dmlc::StackTrace() + 301\\n[bt] (1) 1   libxgboost.dylib                    0x000000201a35e15f dmlc::LogMessageFatal::~LogMessageFatal() + 47\\n[bt] (2) 2   libxgboost.dylib                    0x000000201a35dc19 dmlc::LogMessageFatal::~LogMessageFatal() + 9\\n[bt] (3) 3   libxgboost.dylib                    0x000000201a3e3638 xgboost::ObjFunction::Create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 1000\\n[bt] (4) 4   libxgboost.dylib                    0x000000201a37076e xgboost::LearnerImpl::LazyInitModel() + 718\\n[bt] (5) 5   libxgboost.dylib                    0x000000201a37c60b XGBoosterUpdateOneIter + 91\\n[bt] (6) 6   libffi.6.dylib                      0x0000000104e22884 ffi_call_unix64 + 76\\n[bt] (7) 7   ???                                 0x00007ffeeca33440 0x0 + 140732868539456\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-aa1227b47e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#fit model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m#make predictions for the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    545\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1021\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snakes/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[18:03:08] src/objective/objective.cc:23: Unknown objective function reg:squarederror\\n\\nStack trace returned 8 entries:\\n[bt] (0) 0   libxgboost.dylib                    0x000000201a35e3bd dmlc::StackTrace() + 301\\n[bt] (1) 1   libxgboost.dylib                    0x000000201a35e15f dmlc::LogMessageFatal::~LogMessageFatal() + 47\\n[bt] (2) 2   libxgboost.dylib                    0x000000201a35dc19 dmlc::LogMessageFatal::~LogMessageFatal() + 9\\n[bt] (3) 3   libxgboost.dylib                    0x000000201a3e3638 xgboost::ObjFunction::Create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 1000\\n[bt] (4) 4   libxgboost.dylib                    0x000000201a37076e xgboost::LearnerImpl::LazyInitModel() + 718\\n[bt] (5) 5   libxgboost.dylib                    0x000000201a37c60b XGBoosterUpdateOneIter + 91\\n[bt] (6) 6   libffi.6.dylib                      0x0000000104e22884 ffi_call_unix64 + 76\\n[bt] (7) 7   ???                                 0x00007ffeeca33440 0x0 + 140732868539456\\n\\n'"
     ]
    }
   ],
   "source": [
    "param={}\n",
    "\n",
    "param_list = ['objective']\n",
    "random_num_list=['reg:squarederror', 'reg:squaredlogerror', 'reg:logistic',\n",
    "                'reg:pseudohubererror', 'binary:logistic', 'binary:logitraw',\n",
    "                'binary:hinge', 'count:poisson', 'survival:cox', 'survival:aft',\n",
    "                'aft_loss_distribution', 'multi:softmax', 'multi:softprob',\n",
    "                'rank:pairwise', 'rank:ndcg', 'rank:map', 'rank:gamma', ' reg:tweedie']\n",
    "\n",
    "\n",
    "#set at \"binary:logistoc\"\n",
    "param['objective']= \"reg:logistic\"\n",
    "\n",
    "\n",
    "\n",
    "#go throguh all params\n",
    "for p in range(len(param_list)):\n",
    "    for r in range(len(random_num_list)):\n",
    "        params={}  \n",
    "        params[param_list[p]] = random_num_list[r]\n",
    "        print('%s = %s' % (param_list[p], random_num_list[r]))\n",
    "        #training the MBTI type indicators individually\n",
    "        for m in range(len(mbti_type_ind)):\n",
    "            print(\"%s ... \" % (mbti_type_ind[m]))\n",
    "\n",
    "            Y= personalities[:,m]\n",
    "\n",
    "            #split into train and test sets\n",
    "            seed = 50\n",
    "            test_size = 0.50\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "            #fit model on the training data\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            #make predictions for the test data\n",
    "            Y_prediction = model.predict(X_test)\n",
    "            predictions = [round(value) for value in Y_prediction]\n",
    "\n",
    "            #check the accuracy\n",
    "            accuracy = accuracy_score(Y_test, predictions)\n",
    "            print(\" - %s Accuracy: %.2f%%\" % (mbti_type_ind[m], accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
